{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Repo Link](https://github.com/habibaelghazouly/ML-projects.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915d8883"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYmEKXsNbw8z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from src import preprocess_mnist\n",
        "from src import NNModel\n",
        "from src import train_model_nn\n",
        "from src import plot_training_curves\n",
        "from src import detect_convergence, plot_convergence \n",
        "from src.NNs.helpers import get_gradients\n",
        "from src import LogisticRegressionModel\n",
        "from src import train_model, test_model\n",
        "from src import plot_curves, print_confusion_matrix\n",
        "from src import SoftmaxRegressionModel\n",
        "from src import CNNModel\n",
        "from src import train_model_cnn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55322b37"
      },
      "source": [
        "## Displaying Non-Flattened MNIST Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "45TC_uUZeWWW",
        "outputId": "301e87c7-5941-4bca-ffa6-0f3c23b1b45a"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = preprocess_mnist(flatten=False)\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
        "for i in range(8):\n",
        "    axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
        "    axes[i].set_title(str(labels[i].item()))\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf7b623"
      },
      "source": [
        "## Displaying Flattened MNIST Data Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGv4bZFseQAV",
        "outputId": "15ff5b0a-1cfd-4ef5-dfc3-5beb63dc1c80"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = preprocess_mnist(batch_size=64, augment=False, flatten=True)\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"Images batch shape: {images.shape}\")\n",
        "print(f\"Labels batch shape: {labels.shape}\")\n",
        "print(f\"Example labels: {labels[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binary Classification (0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter 0 and 1 only\n",
        "def filter_binary(loader):\n",
        "    X, y = [], []\n",
        "    for img, label in loader.dataset:\n",
        "        if label in [0, 1]:\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "    X = torch.stack(X)\n",
        "    y = torch.tensor(y)\n",
        "    ds = torch.utils.data.TensorDataset(X, y)\n",
        "    return torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
        "\n",
        "train_loader_bin = filter_binary(train_loader)\n",
        "val_loader_bin = filter_binary(val_loader)\n",
        "test_loader_bin = filter_binary(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_log = LogisticRegressionModel(input_dim=784)\n",
        "loss_fn = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Training time\n",
        "start_time = time.time()\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model_log, train_loader_bin, val_loader_bin,\n",
        "    epochs=30, lr=0.01, device=device, loss_fn=loss_fn, binary=True\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time_logistic = end_time - start_time\n",
        "print(f\"Training time for Logistic Regression Model: {training_time_logistic:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_curves(train_losses, val_losses, \"Binary Logistic Regression - Loss\", \"Loss\")\n",
        "plot_curves(train_accs, val_accs, \"Binary Logistic Regression - Accuracy\", \"Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_logistic, cm = test_model(model_log, test_loader_bin, device, binary=True)\n",
        "print(f\"Test Accuracy: {acc_logistic*100:.3f}%\")\n",
        "print_confusion_matrix(cm, classes=[\"0\", \"1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Softmax Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model\n",
        "model_softmax = SoftmaxRegressionModel(input_dim=784, num_classes=10)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training time\n",
        "start_time = time.time()\n",
        "train_loader, val_loader, test_loader = preprocess_mnist(flatten=True)\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model_softmax, train_loader, val_loader,\n",
        "    epochs=10, lr=0.01, device=device, loss_fn=loss_fn\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time_softmax = end_time - start_time\n",
        "print(f\"Training time for Softmax Model: {training_time_softmax:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "plot_curves(train_losses, val_losses, \"Softmax Regression - Loss\", \"Loss\")\n",
        "plot_curves(train_accs, val_accs, \"Softmax Regression - Accuracy\", \"Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "acc_softmax, cm = test_model(model_softmax, test_loader, device)\n",
        "print(f\"Test Accuracy: {acc_softmax*100:.3f}%\")\n",
        "print_confusion_matrix(cm, classes=[str(i) for i in range(10)])\n",
        "\n",
        "# per class accuracy\n",
        "class_correct = [0 for _ in range(10)]\n",
        "class_total = [0 for _ in range(10)]\n",
        "model_softmax.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_softmax(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print(f\"Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Accuracy of class {i}: N/A (no samples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model, loss, optimizer\n",
        "model = NNModel().to(device)\n",
        "model.apply(model._init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "checkpoint_path = \"./checkpoints/mnist.pth\"\n",
        "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "train_loader, val_loader, test_loader = preprocess_mnist(batch_size=64, augment=False, flatten=True)\n",
        "\n",
        "# Train\n",
        "history = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, device=device, checkpoint_path=checkpoint_path)\n",
        "\n",
        "# Plot\n",
        "plot_training_curves(history)\n",
        "conv_epoch = detect_convergence(history[\"val_loss_mean\"])\n",
        "plot_convergence(history[\"train_loss_mean\"], history[\"val_loss_mean\"], conv_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Learning Rate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test values : [0.001, 0.01, 0.1, 1.0]\n",
        "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
        "epochs = 5\n",
        "\n",
        "results_lr = {}\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "    model = NNModel().to(device)\n",
        "    model.apply(model._init_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    result = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, device=device)\n",
        "    results_lr[lr] = result\n",
        "    plot_training_curves(result)\n",
        "    conv_epoch = detect_convergence(result[\"val_loss_mean\"])\n",
        "    plot_convergence(result[\"train_loss_mean\"], result[\"val_loss_mean\"], conv_epoch)\n",
        "\n",
        "# best lr \n",
        "best_lr = None\n",
        "best_acc = 0.0\n",
        "\n",
        "for lr, history in results_lr.items():\n",
        "    val_acc = history[\"val_acc_mean\"][-1]\n",
        "    print(f\"LR {lr:<5} → Final Val Acc: {val_acc*100:.2f}%\")\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_lr = lr\n",
        "\n",
        "print(f\"\\nBest Learning Rate: {best_lr} with Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Batch Size Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the full-batch sample and calculating gradient\n",
        "\n",
        "full_batch_images, full_batch_labels = next(iter(train_loader))\n",
        "full_batch_images, full_batch_labels = full_batch_images.to(device), full_batch_labels.to(device)\n",
        "\n",
        "model = NNModel().to(device)\n",
        "model.apply(model._init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)    \n",
        "full_result = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=5, device=device)\n",
        "\n",
        "full_batch_gradients = get_gradients(model, criterion, full_batch_images, full_batch_labels)\n",
        "\n",
        "# Test Values : [16, 32, 64, 128]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "epochs = 5\n",
        "results_bs = {} \n",
        "grad_noise_results = []\n",
        "\n",
        "for bs in batch_sizes:  \n",
        "    print(f\"Training with batch size: {bs}\")\n",
        "    train_loader_bs, val_loader_bs, test_loader_bs = preprocess_mnist(batch_size=bs, augment=False, flatten=True)\n",
        "   \n",
        "    # model, loss, optimizer\n",
        "    model = NNModel().to(device)\n",
        "    model.apply(model._init_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    result = train_model_nn(model, train_loader_bs, val_loader_bs, criterion, optimizer, epochs=epochs, device=device)\n",
        "    train_time = time.time() - start_time\n",
        "    results_bs[bs] = result\n",
        "\n",
        "    final_val_acc = result[\"val_acc_mean\"][-1]\n",
        " \n",
        "    print(f\"----- Final Val Acc: {final_val_acc*100:.2f}% | Train Time: {train_time:.2f}s -----\")\n",
        "\n",
        "\n",
        "    # Random stochastic batch\n",
        "    batch_inputs, batch_targets = next(iter(DataLoader(train_loader_bs.dataset, batch_size=bs, shuffle=True)))\n",
        "    batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "    \n",
        "    stoch_grads = get_gradients(model, criterion, batch_inputs.view(batch_inputs.size(0), -1), batch_targets)\n",
        "    \n",
        "    # Gradient noise \n",
        "    grad_noise = [sg - tg for sg, tg in zip(stoch_grads, full_batch_gradients)]\n",
        "    noise_norm = torch.sqrt(sum([g.pow(2).sum() for g in grad_noise])).item()\n",
        "    \n",
        "    grad_noise_results.append((bs, final_val_acc * 100, train_time, noise_norm))\n",
        "    print(f\"----- Gradient Noise : {noise_norm:.6f} -----\")\n",
        "\n",
        "\n",
        "summary_df = pd.DataFrame(grad_noise_results, columns=[\"Batch Size\", \"Val Accuracy (%)\", \"Train Time (s)\", \"Gradient Noise\"])\n",
        "display(summary_df)\n",
        "\n",
        " \n",
        "# best batch size\n",
        "best_bs = None\n",
        "best_acc = 0.0  \n",
        "for bs, history in results_bs.items():\n",
        "    val_acc = history[\"val_acc_mean\"][-1]\n",
        "    print(f\"BS {bs:<5} → Final Val Acc: {val_acc*100:.2f}%\")\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_bs = bs\n",
        "\n",
        "\n",
        "#  visualization\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(summary_df[\"Batch Size\"], summary_df[\"Gradient Noise\"], marker='o')\n",
        "plt.title(\"Gradient Noise vs Batch Size (MNIST)\")\n",
        "plt.xlabel(\"Batch Size\")\n",
        "plt.ylabel(\"Gradient Noise\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nBest Batch Size: {best_bs} with Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch size & Gradient Noise are inversely proportioned.\n",
        "As the batch size increases, the noise of gradient estimates decreases, which leads to a smoother curves and more stable convergence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Architecture Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = [2, 3, 4, 5]\n",
        "neurons_per_layer = [64, 128, 256, 512]\n",
        "epochs = 2\n",
        "results_arch = {}\n",
        "\n",
        "for num_layers in layers:\n",
        "    for neurons in neurons_per_layer:\n",
        "        print(f\"Training with {num_layers} layers and {neurons} neurons per layer\")\n",
        "\n",
        "        model = NNModel(hidden_sizes=[neurons]*num_layers).to(device)\n",
        "        model.apply(model._init_weights)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "        \n",
        "        result = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, device=device)\n",
        "      \n",
        "        results_arch[(num_layers, neurons)] = result\n",
        "\n",
        "arch_df = pd.DataFrame(\n",
        "    [(num_layers, neurons, res[\"val_acc_mean\"][-1] * 100,) \n",
        "     for (num_layers, neurons), res in results_arch.items()],\n",
        "    columns=[\"Num Layers\", \"Neurons per Layer\", \"Val Accuracy (%)\"]\n",
        ")\n",
        "display(arch_df)            \n",
        "\n",
        "best_row = arch_df.loc[arch_df[\"Val Accuracy (%)\"].idxmax()]\n",
        "print(f\"\\nBest Architecture: {int(best_row['Num Layers'])} layers × {int(best_row['Neurons per Layer'])} neurons\")\n",
        "print(f\"Validation Accuracy: {best_row['Val Accuracy (%)']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best NN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Architecture Model Training\n",
        "\n",
        "# Best lr , Best bs , Best architecture\n",
        "num_layers = int(best_row[\"Num Layers\"])\n",
        "neurons = int(best_row[\"Neurons per Layer\"])\n",
        "hidden_sizes = [neurons] * num_layers\n",
        "\n",
        "train_loader_best, val_loader_best, test_loader_best = preprocess_mnist(batch_size=best_bs, augment=False, flatten=True)\n",
        "\n",
        "best_model = NNModel(hidden_sizes=hidden_sizes).to(device)\n",
        "best_model.apply(best_model._init_weights)\n",
        "best_criterion = nn.CrossEntropyLoss()\n",
        "best_optimizer = torch.optim.SGD(best_model.parameters(), lr=best_lr)\n",
        "\n",
        "print(f\"Best NN model: {num_layers} layers × {neurons} neurons, lr={best_lr}, batch_size={best_bs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the best NN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train best_model\n",
        "train_time_start = time.time()\n",
        "train_model_nn(best_model, train_loader_best, val_loader_best, best_criterion, best_optimizer, epochs=5, device=device)\n",
        "train_time_end = time.time()    \n",
        "train_time_best_nn = train_time_end - train_time_start\n",
        "print(f\"Training time for Best NN Model: {train_time_best_nn:.2f} seconds with accuracy {best_acc*100:.2f}%\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"Softmax Regression\", \"Neural Network\"],\n",
        "    \"Accuracy (%)\": [acc_logistic * 100, acc_softmax * 100, best_acc * 100],\n",
        "    \"Training Time (s)\": [training_time_logistic, training_time_softmax, train_time_best_nn]\n",
        "})\n",
        "\n",
        "# Round for neat display\n",
        "results_df[\"Accuracy (%)\"] = results_df[\"Accuracy (%)\"].round(2)\n",
        "results_df[\"Training Time (s)\"] = results_df[\"Training Time (s)\"].round(2)\n",
        "\n",
        "# Display\n",
        "display(results_df)\n",
        "\n",
        "# Bar plots\n",
        "fig, ax1 = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1[0].bar(results_df[\"Model\"], results_df[\"Accuracy (%)\"], color=\"steelblue\")\n",
        "ax1[0].set_title(\"Model Accuracy Comparison\")\n",
        "ax1[0].set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "# Time plot\n",
        "ax1[1].bar(results_df[\"Model\"], results_df[\"Training Time (s)\"], color=\"steelblue\")\n",
        "ax1[1].set_title(\"Model Training Time Comparison\")\n",
        "ax1[1].set_ylabel(\"Training Time (seconds)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### **Computational Complexity & Training Time**\n",
        "- **Logistic Regression:** Fastest and simplest model. It trains quickly since it only involves one linear layer.  \n",
        "- **Softmax Regression:** Slightly heavier than logistic, but still efficient and quick to converge.  \n",
        "- **Neural Network:** Most computationally complex as deeper layers and more parameters make training slower but more powerful.\n",
        "\n",
        "\n",
        "\n",
        "##### **When to Use Each**\n",
        "- **Logistic Regression:** Best for simple, linearly separable problems or when speed and interpretability matter most.  \n",
        "- **Softmax Regression:** Great for multi-class problems with mostly linear relationships.  \n",
        "- **Neural Network:** Ideal for complex, non-linear data like images , use it when accuracy is more important than speed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test best nn model\n",
        "best_nn_acc, cm = test_model(best_model, test_loader_best, device)\n",
        "print(f\"Best NN Model Test Accuracy: {best_nn_acc*100:.3f}%\")\n",
        "print_confusion_matrix(cm, classes=[str(i) for i in range(10)])\n",
        "\n",
        "# Analysis of misclassifications\n",
        "misclassified_indices = []\n",
        "for images, labels in test_loader_best:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = best_model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    for i in range(len(labels)):\n",
        "        if predicted[i] != labels[i]:\n",
        "            misclassified_indices.append((images[i].cpu(), labels[i].cpu(), predicted[i].cpu()))\n",
        "\n",
        "# Display some misclassified samples\n",
        "num_to_display = 8\n",
        "fig, axes = plt.subplots(1, num_to_display, figsize=(15, 3))\n",
        "for i in range(num_to_display): \n",
        "    if i < len(misclassified_indices):\n",
        "        img, true_label, pred_label = misclassified_indices[i]\n",
        "        axes[i].imshow(img.view(28, 28), cmap='gray')\n",
        "        axes[i].set_title(f\"T: {true_label.item()} P: {pred_label.item()}\")\n",
        "        axes[i].axis('off')            \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnn = CNNModel(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()   \n",
        "optimizer = torch.optim.SGD(model_cnn.parameters(), lr=0.01)\n",
        "\n",
        "train_loader, val_loader, test_loader = preprocess_mnist(batch_size=64, augment=True, flatten=False)\n",
        "epochs = 10\n",
        "    \n",
        "train_model_cnn(\n",
        "    model=model_cnn,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    epochs=epochs,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnn.eval()\n",
        "correct = 0 \n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "acc_cnn = 100 * correct / total\n",
        "print(f'Test Accuracy: {acc_cnn:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN vs Fully-connected Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# performance comparison\n",
        "models = ['Best NN Model', 'CNN Model']\n",
        "accuracies = [best_nn_acc * 100, acc_cnn]\n",
        "\n",
        "#summary table\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Model\": models,\n",
        "    \"Accuracy (%)\": accuracies\n",
        "})\n",
        "\n",
        "display(summary_df) \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
