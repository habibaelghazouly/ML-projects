{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Repo Link](https://github.com/habibaelghazouly/ML-projects.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915d8883"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYmEKXsNbw8z"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from src import preprocess_mnist\n",
        "from src import NNModel\n",
        "from src import train_model_nn\n",
        "from src import plot_training_curves\n",
        "from src import detect_convergence, plot_convergence \n",
        "from src.NNs.helpers import get_gradients\n",
        "from src import LogisticRegressionModel\n",
        "from src import train_model, test_model\n",
        "from src import plot_curves, print_confusion_matrix\n",
        "from src import SoftmaxRegressionModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55322b37"
      },
      "source": [
        "## Displaying Non-Flattened MNIST Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "45TC_uUZeWWW",
        "outputId": "301e87c7-5941-4bca-ffa6-0f3c23b1b45a"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = preprocess_mnist(flatten=False)\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
        "for i in range(8):\n",
        "    axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
        "    axes[i].set_title(str(labels[i].item()))\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf7b623"
      },
      "source": [
        "## Displaying Flattened MNIST Data Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGv4bZFseQAV",
        "outputId": "15ff5b0a-1cfd-4ef5-dfc3-5beb63dc1c80"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = preprocess_mnist(batch_size=64, augment=False, flatten=True)\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"Images batch shape: {images.shape}\")\n",
        "print(f\"Labels batch shape: {labels.shape}\")\n",
        "print(f\"Example labels: {labels[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter 0 and 1 only\n",
        "def filter_binary(loader):\n",
        "    X, y = [], []\n",
        "    for img, label in loader.dataset:\n",
        "        if label in [0, 1]:\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "    X = torch.stack(X)\n",
        "    y = torch.tensor(y)\n",
        "    ds = torch.utils.data.TensorDataset(X, y)\n",
        "    return torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
        "\n",
        "train_loader_bin = filter_binary(train_loader)\n",
        "val_loader_bin = filter_binary(val_loader)\n",
        "test_loader_bin = filter_binary(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_log = LogisticRegressionModel(input_dim=784)\n",
        "loss_fn = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model_log, train_loader_bin, val_loader_bin,\n",
        "    epochs=30, lr=0.01, device=device, loss_fn=loss_fn, binary=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_curves(train_losses, val_losses, \"Binary Logistic Regression - Loss\", \"Loss\")\n",
        "plot_curves(train_accs, val_accs, \"Binary Logistic Regression - Accuracy\", \"Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc, cm = test_model(model_log, test_loader_bin, device, binary=True)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print_confusion_matrix(cm, classes=[\"0\", \"1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Softmax Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model\n",
        "model_softmax = SoftmaxRegressionModel(input_dim=784, num_classes=10)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = preprocess_mnist(flatten=True)\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model_softmax, train_loader, val_loader,\n",
        "    epochs=30, lr=0.01, device=device, loss_fn=loss_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "plot_curves(train_losses, val_losses, \"Softmax Regression - Loss\", \"Loss\")\n",
        "plot_curves(train_accs, val_accs, \"Softmax Regression - Accuracy\", \"Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "acc, cm = test_model(model_softmax, test_loader, device)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print_confusion_matrix(cm, classes=[str(i) for i in range(10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model, loss, optimizer\n",
        "model = NNModel().to(device)\n",
        "model.apply(model._init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "checkpoint_path = \"./checkpoints/mnist.pth\"\n",
        "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "train_loader, val_loader, test_loader = preprocess_mnist(batch_size=64, augment=False, flatten=True)\n",
        "\n",
        "# Train\n",
        "history = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, device=device, checkpoint_path=checkpoint_path)\n",
        "\n",
        "# Plot\n",
        "plot_training_curves(history)\n",
        "conv_epoch = detect_convergence(history[\"val_loss_mean\"])\n",
        "plot_convergence(history[\"train_loss_mean\"], history[\"val_loss_mean\"], conv_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Learning Rate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test values : [0.001, 0.01, 0.1, 1.0]\n",
        "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
        "\n",
        "results_lr = {}\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "    model = NNModel().to(device)\n",
        "    model.apply(model._init_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    result = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, device=device)\n",
        "    results_lr[lr] = result\n",
        "    plot_training_curves(result)\n",
        "    conv_epoch = detect_convergence(result[\"val_loss_mean\"])\n",
        "    plot_convergence(result[\"train_loss_mean\"], result[\"val_loss_mean\"], conv_epoch)\n",
        "\n",
        "# best lr \n",
        "best_lr = None\n",
        "best_acc = 0.0\n",
        "\n",
        "for lr, history in results_lr.items():\n",
        "    val_acc = history[\"val_acc_mean\"][-1]\n",
        "    print(f\"LR {lr:<5} → Final Val Acc: {val_acc*100:.2f}%\")\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_lr = lr\n",
        "\n",
        "print(f\"\\nBest Learning Rate: {best_lr} with Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Batch Size Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the full-batch sample and calculating gradient\n",
        "\n",
        "full_batch_images, full_batch_labels = next(iter(train_loader))\n",
        "full_batch_images, full_batch_labels = full_batch_images.to(device), full_batch_labels.to(device)\n",
        "\n",
        "model = NNModel().to(device)\n",
        "model.apply(model._init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)    \n",
        "full_result = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=10, device=device)\n",
        "\n",
        "full_batch_gradients = get_gradients(model, criterion, full_batch_images, full_batch_labels)\n",
        "\n",
        "# Test Values : [16, 32, 64, 128]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "epochs = 10\n",
        "results_bs = {} \n",
        "grad_noise_results = []\n",
        "\n",
        "for bs in batch_sizes:  \n",
        "    print(f\"Training with batch size: {bs}\")\n",
        "    train_loader_bs, val_loader_bs, test_loader_bs = preprocess_mnist(batch_size=bs, augment=False, flatten=True)\n",
        "   \n",
        "    # model, loss, optimizer\n",
        "    model = NNModel().to(device)\n",
        "    model.apply(model._init_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    result = train_model_nn(model, train_loader_bs, val_loader_bs, criterion, optimizer, epochs=epochs, device=device)\n",
        "    train_time = time.time() - start_time\n",
        "    results_bs[bs] = result\n",
        "\n",
        "    final_val_acc = result[\"val_acc_mean\"][-1]\n",
        " \n",
        "    print(f\"----- Final Val Acc: {final_val_acc*100:.2f}% | Train Time: {train_time:.2f}s -----\")\n",
        "\n",
        "\n",
        "    # Random stochastic batch\n",
        "    batch_inputs, batch_targets = next(iter(DataLoader(train_loader_bs.dataset, batch_size=bs, shuffle=True)))\n",
        "    batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "    \n",
        "    stoch_grads = get_gradients(model, criterion, batch_inputs.view(batch_inputs.size(0), -1), batch_targets)\n",
        "    \n",
        "    # Gradient noise \n",
        "    grad_noise = [sg - tg for sg, tg in zip(stoch_grads, full_batch_gradients)]\n",
        "    noise_norm = torch.sqrt(sum([g.pow(2).sum() for g in grad_noise])).item()\n",
        "    \n",
        "    grad_noise_results.append((bs, final_val_acc * 100, train_time, noise_norm))\n",
        "    print(f\"----- Gradient Noise : {noise_norm:.6f} -----\")\n",
        "\n",
        "\n",
        "summary_df = pd.DataFrame(grad_noise_results, columns=[\"Batch Size\", \"Val Accuracy (%)\", \"Train Time (s)\", \"Gradient Noise\"])\n",
        "display(summary_df)\n",
        "\n",
        " \n",
        "# best batch size\n",
        "best_bs = None\n",
        "best_acc = 0.0  \n",
        "for bs, history in results_bs.items():\n",
        "    val_acc = history[\"val_acc_mean\"][-1]\n",
        "    print(f\"BS {bs:<5} → Final Val Acc: {val_acc*100:.2f}%\")\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_bs = bs\n",
        "\n",
        "\n",
        "#  visualization\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(summary_df[\"Batch Size\"], summary_df[\"Gradient Noise\"], marker='o')\n",
        "plt.title(\"Gradient Noise vs Batch Size (MNIST)\")\n",
        "plt.xlabel(\"Batch Size\")\n",
        "plt.ylabel(\"Gradient Noise\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nBest Batch Size: {best_bs} with Val Acc = {best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Architecture Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = [2, 3, 4, 5]\n",
        "neurons_per_layer = [64, 128, 256, 512]\n",
        "epochs = 2\n",
        "results_arch = {}\n",
        "\n",
        "for num_layers in layers:\n",
        "    for neurons in neurons_per_layer:\n",
        "        print(f\"Training with {num_layers} layers and {neurons} neurons per layer\")\n",
        "\n",
        "        model = NNModel(hidden_sizes=[neurons]*num_layers).to(device)\n",
        "        model.apply(model._init_weights)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "        \n",
        "        result = train_model_nn(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, device=device)\n",
        "      \n",
        "        results_arch[(num_layers, neurons)] = result\n",
        "\n",
        "arch_df = pd.DataFrame(\n",
        "    [(num_layers, neurons, res[\"val_acc_mean\"][-1] * 100,) \n",
        "     for (num_layers, neurons), res in results_arch.items()],\n",
        "    columns=[\"Num Layers\", \"Neurons per Layer\", \"Val Accuracy (%)\"]\n",
        ")\n",
        "display(arch_df)            \n",
        "\n",
        "best_row = arch_df.loc[arch_df[\"Val Accuracy (%)\"].idxmax()]\n",
        "print(f\"\\nBest Architecture: {int(best_row['Num Layers'])} layers × {int(best_row['Neurons per Layer'])} neurons\")\n",
        "print(f\"Validation Accuracy: {best_row['Val Accuracy (%)']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ml-env)",
      "language": "python",
      "name": "ml-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
